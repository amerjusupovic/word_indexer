{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer_fit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gmihaila/word_indexer/blob/master/tokenizer_fit.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "bPFsMJobv2NA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to index words in a large dataset using Keras Tokenizer"
      ]
    },
    {
      "metadata": {
        "id": "lYRnyodQ5Cif",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Indexer\n",
        "\n",
        "* This is a very helpful and easy to use function that can index words or characters when you have Big Data.\n",
        "* It batches the data, so is VERY fast and uses very little memory\n",
        "* Returns Keras type Tokenizer\n",
        "* It uses the keras Tokenizer: \n",
        "\n",
        "  * **num_words**: the maximum number of words to keep, based on word frequency. Only the most common num_words *  *  will be kept.\n",
        "  * **filters**: a string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "  * **lower**: boolean. Whether to convert the texts to lowercase.\n",
        "  * **split**: str. Separator for word splitting.\n",
        "  * **char_level**: if True, every character will be treated as a token.\n",
        "  * **oov_token**: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
        "\n",
        "\n",
        "[Keras Text Preprocessing](https://keras.io/preprocessing/text/)\n",
        "\n",
        "\n",
        "### Function Setup\n",
        "\n",
        "```\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def FitTokenizer(data, n_words, batch_size, data_size):\n",
        "\n",
        "      data - any array of strings. It can even be a generator\n",
        "   n_words - how many words to use when indexing\n",
        "batch_size - any value smaller than data size \n",
        " data_size - how many strings you have in your data\n",
        " \n",
        "```\n",
        "**NOTE:** A string is treated as a document of text.\n",
        " \n",
        "\n",
        "```\n",
        "tk = FitTokenizer(my_data, n_words, batch_size, len(my_data))\n",
        "\n",
        "tk.word_counts - dictionary word : word count\n",
        "tk.document_count - number of documents (should match data size)\n",
        "tk.word_index - dictionary word : index ascending order from moest frequent\n",
        "tk.word_docs - dictionary word : how many documents it appears in\n",
        "```\n",
        "\n",
        "## Please see [tokenizer_fit.ipynb](https://github.com/gmihaila/word_indexer/blob/master/tokenizer_fit.ipynb)\n",
        "\n",
        "**Thank you for using my code!**\n",
        "\n",
        "Check out more useful tools: \n",
        "[GeorgeM](https://gmihaila.github.io)"
      ]
    },
    {
      "metadata": {
        "id": "cc84mcoxgABu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ec7ee58-6562-4fdb-d1d1-7f97e9ad8ec3"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "z67KVo2wv-hK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom function that can batch data if you have large corpus\n",
        "\n",
        "```\n",
        "def FitTokenizer(data, n_words, batch_size, data_size):\n",
        "\n",
        "      data - any array of strings. It can even be a generator\n",
        "   n_words - how many words to use when indexing\n",
        "batch_size - any value smaller than data size \n",
        " data_size - how many strings you have in your data\n",
        "```\n",
        "\n",
        "\n",
        "**NOTE**: A string is treated as a document of text. "
      ]
    },
    {
      "metadata": {
        "id": "9cbkTMLwmQiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6266099c-d31e-460a-d2bd-e151ed9669f3"
      },
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def FitTokenizer(data, n_words, batch_size, data_size):\n",
        "  index = 0\n",
        "  batch = []\n",
        "  # fit tokenizer \n",
        "  tokenizer = Tokenizer(num_words=n_words, filters=string.punctuation, lower=True, split=' ', char_level=False, oov_token='<unk>')\n",
        "  for x in data:\n",
        "    batch.append(x)\n",
        "    if ((index% batch_size) == 0) or (data_size == index):\n",
        "          # feed each sequence\n",
        "          tokenizer.fit_on_texts(batch)\n",
        "          # reset batch\n",
        "          batch = []\n",
        "    index += 1\n",
        "  if n_words is not None:\n",
        "    # fix tokenizer   \n",
        "    tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= n_words}\n",
        "    tokenizer.word_index[tokenizer.oov_token] = n_words + 1\n",
        "\n",
        "  return tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1PUde2OwPWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test it on toy example"
      ]
    },
    {
      "metadata": {
        "id": "XsfL5a18niwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "22aa49b4-82e6-4484-ae9f-80742c473de3"
      },
      "cell_type": "code",
      "source": [
        "my_data = [\"My name is George.\", \"My name is Mhaela\",\"My name is Ionut\"]\n",
        "\n",
        "t = FitTokenizer(data=my_data, n_words=2, batch_size=2, data_size=len(my_data))\n",
        "\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('my', 3), ('name', 3), ('is', 3), ('george', 1), ('mhaela', 1), ('ionut', 1)])\n",
            "3\n",
            "{'my': 1, 'name': 2, '<unk>': 3}\n",
            "{'name': 3, 'mhaela': 1, 'george': 1, 'ionut': 1, 'my': 3, 'is': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzIvy4LEysMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Works the same on Data Generators"
      ]
    },
    {
      "metadata": {
        "id": "UR-YVjPouSvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e802b905-d5a1-4302-9883-01f8f99e7f07"
      },
      "cell_type": "code",
      "source": [
        "# toy example of data generator\n",
        "def DataGen(data):\n",
        "  for x in data:\n",
        "    yield x\n",
        "    \n",
        "generator = DataGen(my_data)\n",
        "t = FitTokenizer(data=generator, n_words=2, batch_size=2, data_size=3)\n",
        "\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('my', 3), ('name', 3), ('is', 3), ('george', 1), ('mhaela', 1), ('ionut', 1)])\n",
            "3\n",
            "{'my': 1, 'name': 2, '<unk>': 3}\n",
            "{'name': 3, 'mhaela': 1, 'george': 1, 'ionut': 1, 'my': 3, 'is': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}